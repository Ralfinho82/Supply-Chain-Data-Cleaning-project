{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import libraries and set display options\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define paths to the CSV files and read them into DataFrames\n",
    "\n",
    "# Paths to the CSV-files\n",
    "data_sample_path = \"data_sample_AufgDataScience_Case.csv\"\n",
    "event_locations_path = \"event_locations_DataScience_Case.csv\"\n",
    "\n",
    "# Read the files into Pandas Dataframes\n",
    "data_sample = pd.read_csv(data_sample_path, sep=\";\")\n",
    "event_locations = pd.read_csv(event_locations_path, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ContainerNumber  ShipmentNumber    PoCreationDate     Material  \\\n",
      "0     HLXU8170197         4868820  19.02.2018 00:00  A2760101414   \n",
      "1     HLXU8170197         4868820  19.02.2018 00:00  A2760101414   \n",
      "2     HLXU8170197         4868820  19.02.2018 00:00  A2760101414   \n",
      "3     HLXU8170197         4868820  19.02.2018 00:00  A2760101414   \n",
      "4     HLXU8170197         4868820  19.02.2018 00:00  A2760101414   \n",
      "\n",
      "   ShipmentQuantity Controller        EventName         EventTime  \\\n",
      "0              72.0        T15     Loading Ship  07.04.2018 13:58   \n",
      "1              72.0        T15   Departure Ship  07.04.2018 19:05   \n",
      "2              72.0        T15   Unloading Ship  21.04.2018 06:19   \n",
      "3              72.0        T15  Departure Truck  21.04.2018 11:42   \n",
      "4              72.0        T15    Arrival Truck  08.05.2018 18:12   \n",
      "\n",
      "   EventMessageTime             EventLocation   VesselName PortofDischarge  \\\n",
      "0  08.04.2018 03:19  urn:jaif:id:loc:26LNLRTM  NYK ROMULUS             NaN   \n",
      "1  09.04.2018 16:29  urn:jaif:id:loc:26LNLRTM  NYK ROMULUS             NaN   \n",
      "2  21.04.2018 11:59  urn:jaif:id:loc:26LUSCHS  NYK ROMULUS             NaN   \n",
      "3  21.04.2018 13:29  urn:jaif:id:loc:26LUSCHS  NYK ROMULUS             NaN   \n",
      "4  23.05.2018 22:04  urn:jaif:id:loc:26LUSCHS  NYK ROMULUS             NaN   \n",
      "\n",
      "   Time2Arrival  \n",
      "0         125.0  \n",
      "1         125.0  \n",
      "2         111.0  \n",
      "3         111.0  \n",
      "4          94.0  \n",
      "              EventLocation                  Bezeichnung\n",
      "0  urn:jaif:id:loc:26LNLRTM              Hafen Rotterdam\n",
      "1  urn:jaif:id:loc:26LUSCHS             Hafen Charleston\n",
      "2  urn:jaif:id:loc:26LDESPE  Consolidation Center Speyer\n",
      "3  urn:jaif:id:loc:26LBEANR              Hafen Antwerpen\n",
      "4  urn:jaif:id:loc:26LUSSAV        Bahnterminal Savannah\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Initial examination of the data\n",
    "\n",
    "# Show first five rows to check contents of Dataframes\n",
    "print(data_sample.head())\n",
    "print(event_locations.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of double rows:  138\n",
      "The original dataframe has 35306 rows.\n",
      "The dataframe without duplicates has 35030 rows.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Count and drop duplicate entries\n",
    "\n",
    "# Count and drop duplicate entries\n",
    "data_sample_duplicates = data_sample.duplicated().sum()\n",
    "print(\"Count of double rows: \", data_sample_duplicates)\n",
    "\n",
    "row_count_original_df = len(data_sample)\n",
    "print(\"The original dataframe has\", row_count_original_df, \"rows.\")\n",
    "\n",
    "data_sample_no_duplicates = data_sample.drop_duplicates(keep=False)\n",
    "row_count_no_duplicates_df = len(data_sample_no_duplicates)\n",
    "print(\"The dataframe without duplicates has\", row_count_no_duplicates_df, \"rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of missing values \n",
      " ContainerNumber         0\n",
      "ShipmentNumber          0\n",
      "PoCreationDate          0\n",
      "Material                0\n",
      "ShipmentQuantity       17\n",
      "Controller              0\n",
      "EventName               0\n",
      "EventTime               0\n",
      "EventMessageTime        0\n",
      "EventLocation         473\n",
      "VesselName              0\n",
      "PortofDischarge     14085\n",
      "Time2Arrival        14113\n",
      "dtype: int64\n",
      "Percent of missing data: 6.3\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Analyse missing values\n",
    "\n",
    "data_sample_count = data_sample_no_duplicates.isnull().sum()\n",
    "percent_missing = round(((data_sample_count.sum() / np.prod(data_sample_no_duplicates.shape)) * 100), 2)\n",
    "# Counting and printing the amount of missing values\n",
    "print(\"Count of missing values\", \"\\n\", data_sample_count)\n",
    "# Printing the percentage of missing values\n",
    "print(\"Percent of missing data:\", percent_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Merge DataFrames and clean up columns\n",
    "\n",
    "# Merge Dataframes on column \"EventLocation\" and drop column \"EventLocation\"\n",
    "merged_data = pd.merge(data_sample_no_duplicates, event_locations, on=\"EventLocation\", how=\"left\")\n",
    "merged_data = merged_data.drop(columns=[\"EventLocation\"])\n",
    "# Rename column \"Bezeichnung\" to \"Description\"\n",
    "merged_data.rename(columns={\"Bezeichnung\": \"Description\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Deal with cryptic codes in columns \"Description\" and \"PortofDischarge\"\n",
    "\n",
    "# Replace values in column \"PortofDischarge\" with the corresponding location names\n",
    "merged_data[\"PortofDischarge\"] = merged_data[\"PortofDischarge\"].replace({\n",
    "    \"urn:jaif:id:loc:26LUSCHS\": \"Hafen Charleston\",\n",
    "    \"urn:jaif:id:loc:26LUSSAV\": \"Bahnterminal Savannah\",\n",
    "    \"urn:jaif:id:loc:26LMXATM\": \"Hafen Altamira\"\n",
    "})\n",
    "# Assigning datatype \"str\" to \"PortofDischarge\"\n",
    "merged_data[\"PortofDischarge\"] = merged_data[\"PortofDischarge\"].astype(str)\n",
    "# Replace empty values and Whitespace in column PortofDischarge with NaN\n",
    "merged_data[\"PortofDischarge\"].replace(\"\", np.nan, inplace=True)\n",
    "merged_data[\"PortofDischarge\"].replace(\" \", np.nan, inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ContainerNumber  ShipmentNumber    PoCreationDate     Material  \\\n",
      "0     HLXU8170197         4868820  19.02.2018 00:00  A2760101414   \n",
      "1     HLXU8170197         4868820  19.02.2018 00:00  A2760101414   \n",
      "2     HLXU8170197         4868820  19.02.2018 00:00  A2760101414   \n",
      "3     HLXU8170197         4868820  19.02.2018 00:00  A2760101414   \n",
      "4     HLXU8170197         4868820  19.02.2018 00:00  A2760101414   \n",
      "\n",
      "   ShipmentQuantity Controller        EventName         EventTime  \\\n",
      "0              72.0        T15     Loading Ship  07.04.2018 13:58   \n",
      "1              72.0        T15   Departure Ship  07.04.2018 19:05   \n",
      "2              72.0        T15   Unloading Ship  21.04.2018 06:19   \n",
      "3              72.0        T15  Departure Truck  21.04.2018 11:42   \n",
      "4              72.0        T15    Arrival Truck  08.05.2018 18:12   \n",
      "\n",
      "   EventMessageTime   VesselName PortofDischarge  Time2Arrival  \\\n",
      "0  08.04.2018 03:19  NYK ROMULUS             nan         125.0   \n",
      "1  09.04.2018 16:29  NYK ROMULUS             nan         125.0   \n",
      "2  21.04.2018 11:59  NYK ROMULUS             nan         111.0   \n",
      "3  21.04.2018 13:29  NYK ROMULUS             nan         111.0   \n",
      "4  23.05.2018 22:04  NYK ROMULUS             nan          94.0   \n",
      "\n",
      "        Description  \n",
      "0   Hafen Rotterdam  \n",
      "1   Hafen Rotterdam  \n",
      "2  Hafen Charleston  \n",
      "3  Hafen Charleston  \n",
      "4  Hafen Charleston  \n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Display the updated DataFrame\n",
    "print(merged_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No invalid characters in column PoCreationDate\n",
      "No invalid characters in column EventTime\n",
      "Invalid characters found in column EventMessageTime. Remove affected lines.\n",
      "Number of lines after removal:35024\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Cleanup date columns - deal with corrupted data\r\n",
    "\r\n",
    "# Deal with unreadable values in column date columns\r\n",
    "# Check if \"#\" is part of the three date columns and remove rows with this sign\r\n",
    "# Regular expression that checks for value \"#\"\r\n",
    "regular_expression_pattern = \"#\"\r\n",
    "\r\n",
    "# Check in each of the three columns and remove the rows with \"#\" characters\r\n",
    "for column in [\"PoCreationDate\", \"EventTime\", \"EventMessageTime\"]:\r\n",
    "    if column in merged_data.columns:\r\n",
    "        contains_invalid_chars = merged_data[column].astype(str).str.contains(regular_expression_pattern)\r\n",
    "        if contains_invalid_chars.any():\r\n",
    "            print(f\"Invalid characters found in column {column}. Remove affected lines.\")\r\n",
    "            merged_data = merged_data[~contains_invalid_chars]\r\n",
    "        else:\r\n",
    "            print(f\"No invalid characters in column {column}\")\r\n",
    "    else:\r\n",
    "        print(f\"Column {column} does not exist in the DataFrame.\")\r\n",
    "\r\n",
    "print(f\"Number of lines after removal:{len(merged_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced NaN values in 'ShipmentQuantity' for 'A2560107300' with: 35\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Deal with missing values in \"ShipmentQuantity\"\r\n",
    "\r\n",
    "# Deal with empty values of column ShipmentQuantity - Exploration reveals, all empty values can be linked to Material value \"A2560107300\"\r\n",
    "# Calculate the mean of \"ShipmentQuantity\" for \"A2560107300\", excluding NaN values in the calculation\r\n",
    "mean_shipment_quantity = merged_data.loc[merged_data[\"Material\"] == \"A2560107300\", \"ShipmentQuantity\"].mean()\r\n",
    "\r\n",
    "# Round the mean to the nearest integer if it is a valid number\r\n",
    "mean_shipment_quantity = round(mean_shipment_quantity)\r\n",
    "\r\n",
    "# Replace NaN values in \"ShipmentQuantity\" for \"A2560107300\" with the mean\r\n",
    "merged_data.loc[(merged_data[\"Material\"] == \"A2560107300\") & (merged_data[\"ShipmentQuantity\"].isna()), \"ShipmentQuantity\"] = mean_shipment_quantity\r\n",
    "print(\"Replaced NaN values in 'ShipmentQuantity' for 'A2560107300' with:\", mean_shipment_quantity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Additional data cleanup and type assignment\n",
    "merged_data[\"PoCreationDate\"] = pd.to_datetime(merged_data[\"PoCreationDate\"], format=\"%d.%m.%Y %H:%M\", errors='coerce')\n",
    "merged_data[\"EventTime\"] = pd.to_datetime(merged_data[\"EventTime\"], format=\"%d.%m.%Y %H:%M\", errors='coerce')\n",
    "merged_data[\"EventMessageTime\"] = pd.to_datetime(merged_data[\"EventMessageTime\"], format=\"%d.%m.%Y %H:%M\", errors='coerce')\n",
    "merged_data[\"ContainerNumber\"] = merged_data[\"ContainerNumber\"].astype(str)\n",
    "merged_data[\"VesselName\"] = merged_data[\"VesselName\"].astype(str)\n",
    "merged_data[\"ShipmentNumber\"] = merged_data[\"ShipmentNumber\"].astype(int)\n",
    "merged_data[\"ShipmentQuantity\"] = merged_data[\"ShipmentQuantity\"].astype(int)\n",
    "merged_data[\"Material\"] = merged_data[\"Material\"].astype(str)\n",
    "merged_data[\"Controller\"] = merged_data[\"Controller\"].astype(str)\n",
    "merged_data[\"EventName\"] = merged_data[\"EventName\"].astype(str)\n",
    "merged_data[\"Description\"] = merged_data[\"Description\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         Hafen Rotterdam\n",
      "1         Hafen Rotterdam\n",
      "2        Hafen Charleston\n",
      "3        Hafen Charleston\n",
      "4        Hafen Charleston\n",
      "               ...       \n",
      "35023    Hafen Charleston\n",
      "35024    Hafen Charleston\n",
      "35025    Hafen Charleston\n",
      "35026     Werk Tuscaloosa\n",
      "35027     Werk Tuscaloosa\n",
      "Name: Description, Length: 35024, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Deal with cryptic codes in column \"EventLocation\"\n",
    "\n",
    "# Deal with value \"urn:jaif:id:loc:25LUN498999044W013\" whis is most likely a typo in column EventLocation. Replace with \"Werk Tuscaloosa\"\n",
    "merged_data[\"Description\"] = merged_data[\"Description\"].replace(\"urn:jaif:id:loc:25LUN498999044W013\", \"Werk Tuscaloosa\")\n",
    "# Check the changes\n",
    "print(merged_data[\"Description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Calculate values for column \"Time2Arrival\"\r\n",
    "\r\n",
    "# Deal with empty values for Time2Arrival - calculate the number\r\n",
    "# Sort the Dataframe based on \"ShipmentNumber\" and \"EventTime\" to ensure the events are in chronological order\r\n",
    "merged_data.sort_values(by=[\"ShipmentNumber\", \"EventTime\"], inplace=True)\r\n",
    "\r\n",
    "# Calculate \"Time2Arrival\" as the number of days from the first event for each \"ShipmentNumber\" to the \"Goods Receipt Dock\" event\r\n",
    "def calculate_time_to_arrival(df):\r\n",
    "    # Calculate the time difference in days from the first event for each \"ShipmentNumber\"\r\n",
    "    first_event_times = df.groupby(\"ShipmentNumber\")[\"EventTime\"].transform(\"max\")\r\n",
    "    df[\"Time2Arrival\"] = -1 * (df[\"EventTime\"] - first_event_times).dt.days\r\n",
    "    return df\r\n",
    "\r\n",
    "# Apply the function to the DataFrame\r\n",
    "merged_data = calculate_time_to_arrival(merged_data)\r\n",
    "\r\n",
    "# Ensure \"Time2Arrival\" is of type int\r\n",
    "merged_data[\"Time2Arrival\"] = merged_data[\"Time2Arrival\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Replace German expression with English epressions \r\n",
    "\r\n",
    "# Replace German with English in \"PortofDischarge\" and \"Description\" columns\r\n",
    "merged_data[\"PortofDischarge\"] = merged_data[\"PortofDischarge\"].str.replace(\"Hafen\", \"Port\")\r\n",
    "merged_data[\"Description\"] = merged_data[\"Description\"].str.replace(\"Hafen\", \"Port\")\r\n",
    "merged_data[\"PortofDischarge\"] = merged_data[\"PortofDischarge\"].str.replace(\"Werk\", \"Plant\")\r\n",
    "merged_data[\"Description\"] = merged_data[\"Description\"].str.replace(\"Werk\", \"Plant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of missing values \n",
      " ContainerNumber     0\n",
      "ShipmentNumber      0\n",
      "PoCreationDate      0\n",
      "Material            0\n",
      "ShipmentQuantity    0\n",
      "Controller          0\n",
      "EventName           0\n",
      "EventTime           0\n",
      "EventMessageTime    0\n",
      "VesselName          0\n",
      "PortofDischarge     0\n",
      "Time2Arrival        0\n",
      "Description         0\n",
      "dtype: int64\n",
      "Percent of missing data: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Final data quality analysis and exporting the DataFrame to CSV\r\n",
    "\r\n",
    "data_sample_count = merged_data.isnull().sum()\r\n",
    "percent_missing = round(((data_sample_count.sum() / np.prod(merged_data.shape)) * 100), 2)\r\n",
    "print(\"Count of missing values\", \"\\n\", data_sample_count)\r\n",
    "print(\"Percent of missing data:\", percent_missing)\r\n",
    "\r\n",
    "merged_data.to_csv(\"exported_data_clean.csv\", index=False, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "name": "python3109jvsc74a57bd0b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}